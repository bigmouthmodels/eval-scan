# ‚ùÄ EvalScan

EvalScan aims to let you quickly understand agent behaviour and locate task bugs through automated scans of your Inspect LLM agent evaluation transcripts.

## Features
* Scan big collections of Inspect logs - `evalscan`'s currently designed to work with DuckDB databases generate by `evaldb`
* Visualize and tabulate agent behaviours and task bugs such as
    * Tool call sequences
    * Length of reasoning messages
    * Refusals and requests for user input
* Generate reports of scan results - see `/examples` for example reports

## Install

```
pip install TODO
```

## Use

**1. Index**: Index your evaluation logs with `evaldb`.

**2. Scan**: Scan your evaluation results with the default probe library:
```
evalscan scan <db_uri>
```
This creates a separate scores table in your database.

**3. Report**: Compile a markdown report summarizing and visualizing scan results via
```
evalscan report <db_uri>
```

## Target Use-cases
* AISI's testing exercises:
    * Quantifying agent behaviours in AISI's CAST evaluations
    * Measuring quality issues in CAST results during AISI's testing exercises
* AISI's CI/CD pipelines:
    * Automated reporting for evaluation suites
    * Automated reporting for agent scaffold
